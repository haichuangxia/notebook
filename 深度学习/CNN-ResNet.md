	ResNet由微软研究院的何凯明等人提出。

## 1. 基本思想

​	随着网络的加深，会出现梯度消失或者梯度爆炸的问题。若传统神经网络能够在n层获得最佳效果，超过则会产生退化现象。那么假设存在一个**恒等映射结构（identity）**，则每个结构可以加深深度，同时又不损失特征，这样可以达到加深网络深度。ResNet中，通过Residual Module(残差模块)实现恒等映射功能。
  假设有恒等映射H（x），但是这个函数比较难算，若假设有H（x）=F（x）+x，那么F（x）=H（x）-x。则有F（x）比H（x）好算。
   残差块之间，通道数相同的构成一个**残差组**。相邻残差组，通道数成倍数关系，1倍或者2倍。
   最后残差组之后接了一个**全局的平均池化**。

## 2. Residual Module（残差块）

###	1. 基本结构

![image-20220417161009108](C:\Users\23860\AppData\Roaming\Typora\typora-user-images\image-20220417161009108.png)

### 2. 两种典型的残差块

- BasicBlock

  - 基本结构

  - ![image-20220417215446550](C:\Users\23860\AppData\Roaming\Typora\typora-user-images\image-20220417215446550.png)

  - 参数计算：
    $$
    \therefore 参数=卷积核尺寸×卷积核深度×卷积核组数（通道数）\\
    带入卷积核大小=3×3，卷积核深度=2，
    $$
    

- BottleBlock

  - 基本结构
    - ![image-20220417215513470](C:\Users\23860\AppData\Roaming\Typora\typora-user-images\image-20220417215513470.png)
  
 - 参数计算

   ​	
   $$
   基本公式：parameters=(卷积核尺寸n^2+1bias)×输入通道数×输出通道数\\
   \therefore c1=1×1×256×64\\
   \therefore c2=3×3×64×64\\
   \therefore c3=1×1×64×256\\
   注意：bias可加可不加，影响不大
   $$
   
 - 基本结构

   - C1（1×1）
     - 使用64个1×1的卷积核，对原来256个通道进行降维

   - C2（3×3）
     - 使用3×3的卷积核进行特征提取，降低特征图的大小，通道数保持不变

   - C3（1×1）
     - 使用256个1×1的卷积核，将64通道增加到256个通道，进行增维



### 3. 结构解读

### 1. shortcut连接
​	如果没有这个shortcut连接，则ResNet就很像VGG，都是**卷积→卷积→池化**的成组的结构。ResNet的创新点就是这个shortcut连接。shortcut起到信息补偿作用，shortcut连接也有权重。

### 2. 激活函数

- 激活函数一般用relu或者leakey ReLu

### 3. 网络的输出为：

​	不是简单加法，而是拼接。是特征矩阵相同位置上的数字相加。
$$
y=F(x)+x
$$
其中x为上一个残差块的输出。要使得尽可能的减少损失，实现恒等映射H(x)，则应该有
$$
H(x)=y=x\\
即有F(x)+x≈x\\
F(x)=H(x)-x\\
即有：F(x)→0
$$
使用Residual Module就是要学习到这样一个恒等H（x）使得在加深深度的同时又不会造成特征的损失。通过实验,发现学习这样一个残差函数F（x）比直接学习恒等映射函数H（x）更容易。F(x)趋向0，则堆叠层的权重趋向0，学习更简单，更方便逼近身份映射。

- 残差的基本概念

$$
H(x)=F(x)+x\\
  则F(x)=H(x)-x
$$
​	H(x):经过残差块之后的输出


- 开始时使用卷积做预热，然后才使用残差块，而不是来就用残差块。

###	2. 引入残差结构的意义

​	传统的卷积神经网络中，随着神经网络深度的加深，很容易出现梯度消失和梯度爆炸的问题。这导致随着深度的增加，神经网络的性能反而可能变差，这总现象就做**退化现象**。使用**正则初始化**对输入进行处理，使用batch normalization操作对**中间层**进行**归一化**操作，保证在反向传播中采用随机梯度下降算法（SGD），从而让网络达到收敛。但是这个方法在网络层数达到上百层时其效果大打折扣。

### 3. 梯度消失问题

​	因为使用了**反向传播算法**，因此**梯度消失往往出现在靠近输入层的地方。**

## 1. 深度残差网络

继承了分组，使用module进行堆叠，这个module叫做residual module。

- 1×1卷积作用
  - 做特征融合
  - 减少参数的个数
  - 计算量主要在反向修正的过程，主要时间都在反向修正权重和偏置，因此减少了参数的个数，训练速度就会大大加快。
## 4. ResNet50的代码实现
### 1.残差块的实现
··· py
# 继承keras.Model类，参数表示父类
class ResBlock（keras.model）:
  # 定义构造函数
  Def __init__(self,channels,strides=1,res_path=False):
    # super父类的构造函数
    super.__init__(ResBlock,self).__init__()
···

