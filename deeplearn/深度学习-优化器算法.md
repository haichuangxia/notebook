​	优化器是深度学习中用于调整**学习率/学习步长**的算法。梯度下降算法是用于调整下学习步长的基本机制，和后面的很多算法都是基于梯度下降算法的变种。

# 1. Gradient Descent（梯度下降）基本机制

# 2. Gradient Descent及其变种算法

## 1. 朴素SGD（Vanila SGD）

## 2. SGD with Momentum

## 3. Nesterov Accelerated Gradient

## 4. Adagrad

## 5. RMSprop

## 6. Adadelta

## 7. Adam

## 8. Nadam

# 3. Reference

[从 SGD 到 Adam —— 深度学习优化算法概览(一)](https://zhuanlan.zhihu.com/p/32626442)