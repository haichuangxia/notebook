### 1.  全连接神经网络与卷积神经网络
####	1. 全连接神经网络

####	2. 卷积神经网络
##### 1. 基本组成
  - 使用卷积层和池化层进行特征提取,相当于 **特征提取器**

  - 使用全连接层(多层感知机MLP)训练参数,计算权重,学习特征之间的非线性组合,是一个**分类器**

##### 2. 卷积网络的基本操作
- 卷积

  ​	基于**感受野**的机制，使用卷积来进行特征提取。

- 非线性激活(ReLu)

  ​	图像分类任务中，更多的是**非线性可分**的问题，因此需要使用非线性的激活函数，增加非线性。

- 池化/下采样

  ​	主要用来提取特征图某些方面的特征，最突出特征，平均特征。同时，池化可以减小特征图的大小。

- 分类(全连接层)
##### 3. 卷积神经网络基本结构
  - 一个卷积神经网络的基本结构


### 2. 卷积神经网络的基本概念

####	1. 感受野
##### 1. 基本概念
###### 1. 感受野
卷积神经网络每一层**输出的特征图（feature map）**上每个像素点在**原始图像**上映射的区域大小，这里的原始图像是指网络的输入图像，是经过预处理（如resize，warp，crop）后的图像。在卷积时，因为使用了一些padding操作，所以计算感受野的时候，需要除掉padding的影响。

神经元感受野的值越大表示其能接触到的原始图像范围就越大，也意味着它可能蕴含更为全局，语义层次更高的特征；相反，值越小则表示其所包含的特征越趋向局部和细节。因此感受野的值可以用来大致判断每一层的抽象层次.

###### 2.  局部感受野
如果只是映射到上一层特征图，则称为**局部感受野**。
##### 2. 感受野有关计算

###### 1. 局部感受野的计算：

​	局部感受野是针对前一层的，局部感受野的大小就等于卷积核的大小
$$
LocalReceptiveField=kernalsize
$$
###### 2. 感受野的计算

- 基本准则：

​				全连接层的感受野为1，最后一层输出特征图的感受野大小等于卷积核大小。

-  感受野的基本计算：

$$
\because n_{out}=\lfloor \frac{n_{in}+2p-f}{s}+1\rfloor\\
\therefore n_{in}=(n_{out}-1)\times s+f-2p\\
假设不存在padding，则有\\
RF_i=(RF_{i+1}-1)\times stride_i+Ksize
$$
##### 3. 感受野大小对分类效果的影响

- 感受野过小
  感受野过小时，主要提取的是局部特征，没有提取全局特征，因此识别的效果会受影响。假设有大小为100×100的原始图像，假设感受野大小为1，也即物体大小的 $$ \frac{1}{10000} $$ ，则智能提取一个像素，无法有效提取物体特征。
- 感受野过大
  感受野过大，提取了很多非物体的特征，导致包含了很多无用的信息，对目标物体的识别进行干扰。假设极端情况，感受野100000倍于待识别物体的大小，则物体的特征就十分明显，寻找物体则无异于大海捞针。

####	2. 卷积/filter
##### 1.  卷积的作用
卷积之后会获得一个特征映射/特征图/feature map,特征图反应了经过卷积之后的图像的特征。

###### 1. 小卷积核

- 优点
  - 参数少，运算量小
  - 特征损失较少
  - 达到与大卷积核相同感受野时，小卷积核数量变多，但是参数和运算量都减小
- 缺点
  - 感受野较小，特征提取不全面

###### 2. 大卷积核

- 优点
  - 特征提取范围较大，感受野也较大
- 缺点
  - 由于卷积核参数的复杂度是$$O(n^2)$$,因此卷积核越大，参数越多，计算量越多

###### 3. 1×1的卷积核

卷积实现特征提取主要是使用**感受野**机制。对于1×1的卷积核，相当于没有提升感受野，因此也就不能实现特征提取。但是其能实现**维度变换**

- 作用
  - 维度变换：利用卷积核的深度改变特征图的深度，实现维度变换
  - 加入非线性：1×1的卷积核可以使用relu等激活函数，添加非线性。
  - 增加网络深度：减少网络模型参数，以便实现深层神经网络。

##### 2.  特征映射
###### 1.  特征映射深度
  **深度** 是<font color="red">卷积操作所用的滤波器的个数.</font>也是卷积后所获特征图的通道数。
###### 2. Convolved Feature
  在原图上滑动滤波器,点乘矩阵所得矩阵称为 ** 卷积特征 ** , ** 激励特征** 或 **特征映射**

##### 3. 卷积参数
- 卷积核的大小

  ​	卷积核的尺寸，一般来说，二维的卷积核是一个（w,d）形式的元组。

- 卷积通道数

  ​	<font color=red>对于某一个卷积层,其使用的卷积核/滤波器的个数</font>

- 步长(Stride)

  ​	<font color=red>每次划过的像素数.步长越大,特征映射越小</font>
##### 4. padding
- 补零(Zero-padding)
  边缘补0,对边缘也添加滤波器.补0也叫做 ** 宽卷积 ** ,若不补0则为 **窄卷积**

#### 3. 池化

##### 1. 作用
- 降低了每个特征映射的维度,但是保留了最重要的信息
- 减少网络参数的个数,防止过拟合

##### 2. 常见的池化方法

- 最大池化

​		获取图像最突出的特征

- 平均池化

​		获取图像共同特征

- 随机池化

##### 3. 全局池化

​	全局池化来源于**network in network**论文中，可以使用全局池化来代替全连接层，这样可以减少参数。

#### 4. 特征图的计算

$$
n_{out}=\lfloor \frac{n_{in}+2p-f}{s}+1\rfloor\\
$$

#### 5. 单通道卷积与多通道卷积

###### 1. 单通道卷积

图像是单通道的输入，一个卷积核只需要对图像进行卷积就可以了。

###### 2. 多通道卷积

一个图像具有多个通道，如原始彩色图像RGB三通道，则一个卷积核需要对RGB三个通道同时卷积，再将卷积所得3个特征进行加和，得到一个新的特征图。

- 一个卷积核对之前的多个通道都要进行卷积
- 有多少个卷积核（卷积核的深度），输出的特征图通道数就有几个。

### 3. 卷积神经网络的基本过程

1. 优化

2. 优化

   1. 局部极小值

      1. 问题

      2. 梯度下降法中解决局部极小值的方法

## 4. 卷积神经网络的参数计算问题

### 1. 卷积过程参数量的计算问题

#### 1. 参数计算的基本原理

卷积核使用**共享权重**的方法，即一个卷积核在单通道的输入上滑动，只有一组权重。

#### 2. 参数的计算基本公式

$$
设卷积核的大小为n×m，图像的通道数为channels，卷积的深度为filters\\
则有：params=n\times m\times channels \times filters
$$

